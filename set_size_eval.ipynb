{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "data_dir = Path('./data')\n",
    "\n",
    "datasets = {\n",
    "    'cifar100': {\n",
    "        'scores': np.load(data_dir / 'clip_probs_matrix_cifar100_2k.npy'),\n",
    "        'targets': np.load(data_dir / 'clip_targets_cifar100_2k.npy'),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numba import njit, set_num_threads\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "#  1) In-place greedy majority rule, using caller-provided scratch arrays\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "@njit(fastmath=True, cache=True)\n",
    "def _greedy_majority_inplace(sorted_idx, n_val, alpha,\n",
    "                             counts, chosen, r_value, order_out, keep_mask):\n",
    "    n_rows, n_cols = sorted_idx.shape\n",
    "\n",
    "    # reset the buffers\n",
    "    for i in range(n_cols):\n",
    "        counts[i] = 0\n",
    "        chosen[i] = False\n",
    "\n",
    "    # greedy picks\n",
    "    for step in range(n_cols):\n",
    "        col_vec = sorted_idx[:, step]\n",
    "        for r in range(n_rows):\n",
    "            counts[col_vec[r]] += 1\n",
    "\n",
    "        # find best\n",
    "        best = -1\n",
    "        best_cnt = -1\n",
    "        for c in range(n_cols):\n",
    "            if (not chosen[c]) and counts[c] > best_cnt:\n",
    "                best_cnt = counts[c]\n",
    "                best = c\n",
    "\n",
    "        chosen[best]    = True\n",
    "        r_value[best]   = (step + 1) / n_cols\n",
    "        order_out[step] = best\n",
    "\n",
    "    # compute boundary on the validation columns\n",
    "    k = math.ceil((n_val + 1) * (1 - alpha))\n",
    "    seen = 0\n",
    "    boundary = 1.0\n",
    "    for step in range(n_cols):\n",
    "        idx = order_out[step]\n",
    "        if idx < n_val:\n",
    "            seen += 1\n",
    "            if seen == k:\n",
    "                boundary = r_value[idx]\n",
    "                break\n",
    "\n",
    "    # build the mask for test columns\n",
    "    for c in range(n_val, n_cols):\n",
    "        keep_mask[c - n_val] = r_value[c] <= boundary\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "#  2) calculate_r_value_fast with full bookkeeping\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "def calculate_r_value_fast(datasets, alpha, num_trials, num_threads=None):\n",
    "    \"\"\"\n",
    "    Same signature as the original, but much faster inside the loop.\n",
    "    Returns:\n",
    "      all_r_value_sets, all_test_targets,\n",
    "      all_sizes_rvalue, all_average_sizes,\n",
    "      all_correct_predictions\n",
    "    \"\"\"\n",
    "    if num_threads is not None:\n",
    "        set_num_threads(num_threads)\n",
    "\n",
    "    all_r_value_sets     = defaultdict(list)\n",
    "    all_sizes_rvalue     = defaultdict(list)\n",
    "    all_average_sizes    = defaultdict(list)\n",
    "    all_correct_predictions = defaultdict(list)\n",
    "\n",
    "    for _ in tqdm(range(num_trials)):\n",
    "        for name, ds in datasets.items():\n",
    "            probs   = ds[\"scores\"]\n",
    "            targets = ds[\"targets\"]\n",
    "\n",
    "            perm       = np.random.permutation(len(probs))\n",
    "            probs_shuf = probs[perm]\n",
    "            targs_shuf = targets[perm]\n",
    "\n",
    "            mid        = len(probs) // 2 \n",
    "            # mid        = int(0.8 * len(probs))\n",
    "            val_p      = probs_shuf[:mid]\n",
    "            tst_p      = probs_shuf[mid:]\n",
    "            val_t      = targs_shuf[:mid]\n",
    "            tst_t      = targs_shuf[mid:]\n",
    "\n",
    "            val_true = val_p[np.arange(mid), :, val_t].T\n",
    "            n_val    = val_true.shape[1]\n",
    "            n_reph   = val_true.shape[0]\n",
    "            n_cls    = probs.shape[2]\n",
    "\n",
    "            full      = np.empty((n_reph, n_val + n_cls), np.float32)\n",
    "            counts    = np.empty(n_val + n_cls,     np.int32)\n",
    "            chosen    = np.empty(n_val + n_cls,     np.bool_)\n",
    "            r_value   = np.empty(n_val + n_cls,   np.float32)\n",
    "            order_out = np.empty(n_val + n_cls,     np.int32)\n",
    "            keep_mask = np.empty(n_cls,            np.bool_)\n",
    "\n",
    "            r_value_sets = []\n",
    "            sizes        = []\n",
    "            hits         = 0\n",
    "\n",
    "            for i in range(len(tst_p)):\n",
    "                full[:, :n_val]  = val_true\n",
    "                full[:, n_val:]  = tst_p[i]\n",
    "\n",
    "                sidx = np.argsort(-full, axis=1).astype(np.int32)\n",
    "\n",
    "                _greedy_majority_inplace(\n",
    "                    sidx, n_val, alpha,\n",
    "                    counts, chosen, r_value, order_out, keep_mask\n",
    "                )\n",
    "\n",
    "                test_rv = r_value[n_val:]          \n",
    "                idxs    = list(np.nonzero(keep_mask)[0]) \n",
    "                rvs     = [float(test_rv[j]) for j in idxs]\n",
    "\n",
    "                r_value_sets.append({\n",
    "                    \"alpha\": alpha,\n",
    "                    \"data\": {\"index\": idxs, \"r_value\": rvs}\n",
    "                })\n",
    "                sizes.append(len(idxs))\n",
    "                if tst_t[i] in idxs:\n",
    "                    hits += 1\n",
    "\n",
    "            all_r_value_sets[name].append(r_value_sets)\n",
    "            all_sizes_rvalue[name].append(sizes)\n",
    "            all_average_sizes[name].append(np.mean(sizes))\n",
    "            all_correct_predictions[name].append(hits / len(tst_p))\n",
    "\n",
    "    print(f'R-VALUE COVERAGE at alpha: {alpha}')\n",
    "    print()\n",
    "    for name, results in all_correct_predictions.items():\n",
    "        print(name.center(50, '-'))\n",
    "        print(f'Coverage: {np.mean(results):.2%} +/- {np.std(results):.2%}')\n",
    "        print()\n",
    "\n",
    "    print('********************')\n",
    "    print(f'SET SIZES at alpha: {alpha}')\n",
    "    print()\n",
    "    for name, results in all_average_sizes.items():\n",
    "        print(name.center(50, '-'))\n",
    "        print(f'Set Size: {np.mean(results):.2f} +/- {np.std(results):.1f}')\n",
    "        print()\n",
    "\n",
    "    return all_average_sizes, all_correct_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def calibrate_lac(scores, targets, alpha=0.1, return_dist=False):\n",
    "    \"\"\"\n",
    "    Estimates the 1-alpha quantile on held-out calibration data.\n",
    "    The score function is `1 - max(softmax_score)`.\n",
    "    \n",
    "    Arguments:\n",
    "        scores: softmax scores of the calibration set\n",
    "        targets: corresponding labels of the calibration set\n",
    "        alpha: parameter for the desired coverage level (1-alpha)\n",
    "\n",
    "    Returns:\n",
    "       qhat: the estimated quantile\n",
    "       score_dist: the score distribution\n",
    "    \"\"\"\n",
    "    scores = torch.tensor(scores, dtype=torch.float)\n",
    "    targets = torch.tensor(targets)\n",
    "    assert scores.size(0) == targets.size(0)\n",
    "    assert targets.size(0)\n",
    "    n = torch.tensor(targets.size(0))\n",
    "    assert n\n",
    "\n",
    "    score_dist = torch.take_along_dim(1 - scores, targets.unsqueeze(1), 1).flatten()\n",
    "    assert (\n",
    "        0 <= torch.ceil((n + 1) * (1 - alpha)) / n <= 1\n",
    "    ), f\"{alpha=} {n=} {torch.ceil((n+1)*(1-alpha))/n=}\"\n",
    "    qhat = torch.quantile(\n",
    "        \n",
    "        score_dist, torch.ceil((n + 1) * (1 - alpha)) / n, interpolation=\"higher\"\n",
    "    )\n",
    "    return (qhat, score_dist) if return_dist else qhat\n",
    "\n",
    "\n",
    "def inference_lac(scores, qhat, allow_empty_sets=False):\n",
    "    \"\"\"\n",
    "    Makes prediction sets on new test data\n",
    "    \n",
    "    Arguments:\n",
    "        scores: softmax scores of the test set\n",
    "        qhat: estimated quantile of the calibration set from the `calirbate_lac` function\n",
    "        allow_empty_sets: if True allow a prediction set to contain no predictions (will then satisfy upper bound of marginal coverage)\n",
    "\n",
    "    Returns:\n",
    "       prediction_sets: boolean mask of prediction sets (True if class is included in the prediction set; otherwise False)\n",
    "    \"\"\"\n",
    "    scores = torch.tensor(scores, dtype=torch.float)\n",
    "    n = scores.size(0)\n",
    "\n",
    "    elements_mask = scores >= (1 - qhat)\n",
    "\n",
    "    if not allow_empty_sets:\n",
    "        elements_mask[torch.arange(n), scores.argmax(1)] = True\n",
    "        \n",
    "    prediction_sets = elements_mask\n",
    "\n",
    "    return prediction_sets\n",
    "\n",
    "\n",
    "\n",
    "def get_coverage(psets, targets, precision=None):\n",
    "    \"\"\"\n",
    "    Calculates empirical coverage of prediction sets\n",
    "    \n",
    "    Arguments:\n",
    "        psets: prediction sets of test set\n",
    "        targets: ground true labels of test set\n",
    "        precision: rounding precision\n",
    "\n",
    "    Returns:\n",
    "       coverage: how many times the answer is in the prediction set\n",
    "    \"\"\"\n",
    "    psets = torch.tensor(psets)\n",
    "    targets = torch.tensor(targets)\n",
    "    psets = psets.clone()\n",
    "    targets = targets.clone()\n",
    "    n = psets.shape[0]\n",
    "    coverage = psets[torch.arange(n), targets].float().mean().item()\n",
    "    # if precision is not None:\n",
    "    #     coverage = round(coverage, precision)\n",
    "    return coverage\n",
    "\n",
    "\n",
    "def get_size(psets, precision=1):\n",
    "    \"\"\"\n",
    "    Calculates empirical set sizes of prediction sets (can consider as the average uncertainty of the model)\n",
    "    \n",
    "    Arguments:\n",
    "        psets: prediction sets of test set\n",
    "        precision: rounding precision\n",
    "\n",
    "    Returns:\n",
    "       size: how many prediction does each set contain on average\n",
    "    \"\"\"\n",
    "    psets = psets.clone()\n",
    "    size = psets.sum(1).float().mean().item()\n",
    "    # if precision is not None:\n",
    "    #     size = round(size, precision)\n",
    "    return size\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_lac(datasets, alpha, num_trials):\n",
    "    all_q_hats = defaultdict(list)\n",
    "    all_scores = defaultdict(list)\n",
    "    all_psets = defaultdict(list)\n",
    "    all_targets = defaultdict(list)\n",
    "    all_coverage = defaultdict(list)\n",
    "    all_size = defaultdict(list)\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        for name, results in datasets.items():\n",
    "            scores = results['scores'][:, 0, :]  \n",
    "            targets = results['targets']\n",
    "            \n",
    "            # shuffle data\n",
    "            index = np.arange(len(scores))\n",
    "            np.random.shuffle(index)\n",
    "\n",
    "            # split data into calibration and test sets\n",
    "            \n",
    "            n = len(scores) // 2\n",
    "            # n = int(0.05 * len(scores))\n",
    "            cal_scores = scores[index][:n]\n",
    "            cal_targets = targets[index][:n]\n",
    "            val_scores = scores[index][n:]\n",
    "            val_targets = targets[index][n:]\n",
    "\n",
    "            # estimate 1-alpha quantile on calibration set\n",
    "            q = calibrate_lac(cal_scores, cal_targets, alpha=alpha)\n",
    "\n",
    "            # make prediction sets on test set\n",
    "            psets = inference_lac(val_scores, q)\n",
    "            \n",
    "            all_psets[name].append(psets)\n",
    "            all_scores[name].append(val_scores)\n",
    "            all_targets[name].append(val_targets)\n",
    "            \n",
    "            # Calculate coverage and set size for this trial\n",
    "            coverage = get_coverage(psets, val_targets)\n",
    "            size = get_size(psets)\n",
    "            \n",
    "            all_coverage[name].append(coverage)\n",
    "            all_size[name].append(size)\n",
    "\n",
    "    print(f'COVERAGE at alpha: {alpha}')\n",
    "    print()\n",
    "\n",
    "    for name in datasets.keys():\n",
    "        print(name.center(50, '-'))\n",
    "        mean_coverage = np.mean(all_coverage[name])\n",
    "        std_coverage = np.std(all_coverage[name])\n",
    "        print(f'{mean_coverage:.2%} +/- {std_coverage:.2%}')\n",
    "        print()\n",
    "\n",
    "    print('********************')\n",
    "    print(f'SET SIZES at alpha: {alpha}')\n",
    "    print()\n",
    "\n",
    "    for name in datasets.keys():\n",
    "        print(name.center(50, '-'))\n",
    "        mean_size = np.mean(all_size[name])\n",
    "        std_size = np.std(all_size[name])\n",
    "        print(f'{mean_size:.2f} +/- {std_size:.1f}')\n",
    "        print()\n",
    "            \n",
    "    return all_coverage, all_size\n",
    "\n",
    "\n",
    "def get_lac_using_mean(datasets, alpha, num_trials):\n",
    "    all_q_hats = defaultdict(list)\n",
    "    all_scores = defaultdict(list)\n",
    "    all_psets = defaultdict(list)\n",
    "    all_targets = defaultdict(list)\n",
    "    all_coverage = defaultdict(list)\n",
    "    all_size = defaultdict(list)\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        for name, results in datasets.items():\n",
    "            scores = results['scores'].mean(axis=1)\n",
    "            targets = results['targets']\n",
    "            \n",
    "            # shuffle data\n",
    "            index = np.arange(len(scores))\n",
    "            np.random.shuffle(index)\n",
    "\n",
    "            # split data into calibration and test sets\n",
    "            \n",
    "            n = len(scores) // 2\n",
    "            # n = int(0.05 * len(scores))\n",
    "            cal_scores = scores[index][:n]\n",
    "            cal_targets = targets[index][:n]\n",
    "            val_scores = scores[index][n:]\n",
    "            val_targets = targets[index][n:]\n",
    "\n",
    "            # estimate 1-alpha quantile on calibration set\n",
    "            q = calibrate_lac(cal_scores, cal_targets, alpha=alpha)\n",
    "\n",
    "            # make prediction sets on test set\n",
    "            psets = inference_lac(val_scores, q)\n",
    "            \n",
    "            all_psets[name].append(psets)\n",
    "            all_scores[name].append(val_scores)\n",
    "            all_targets[name].append(val_targets)\n",
    "            \n",
    "            # Calculate coverage and set size for this trial\n",
    "            coverage = get_coverage(psets, val_targets)\n",
    "            size = get_size(psets)\n",
    "            \n",
    "            all_coverage[name].append(coverage)\n",
    "            all_size[name].append(size)\n",
    "\n",
    "    print(f'MEAN COVERAGE at alpha: {alpha}')\n",
    "    print()\n",
    "\n",
    "    for name in datasets.keys():\n",
    "        print(name.center(50, '-'))\n",
    "        mean_coverage = np.mean(all_coverage[name])\n",
    "        std_coverage = np.std(all_coverage[name])\n",
    "        print(f'{mean_coverage:.2%} +/- {std_coverage:.2%}')\n",
    "        print()\n",
    "\n",
    "    print('********************')\n",
    "    print(f'MEAN SET SIZES at alpha: {alpha}')\n",
    "    print()\n",
    "\n",
    "    for name in datasets.keys():\n",
    "        print(name.center(50, '-'))\n",
    "        mean_size = np.mean(all_size[name])\n",
    "        std_size = np.std(all_size[name])\n",
    "        print(f'{mean_size:.2f} +/- {std_size:.1f}')\n",
    "        print()\n",
    "            \n",
    "    return all_coverage, all_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "\n",
    "    \n",
    "alpha = 0.05  # Desired error rate\n",
    "num_trials = 100  # Number of random trials to perform   \n",
    "\n",
    "    \n",
    "all_sizes_rvalue, all_correct_predictions = calculate_r_value_fast(datasets, alpha, num_trials)\n",
    "all_coverage, all_size = get_lac(datasets, alpha, num_trials)\n",
    "all_coverage_using_mean, all_size_using_mean = get_lac_using_mean(datasets, alpha, num_trials)\n",
    "\n",
    "lac_coverage = defaultdict(list)\n",
    "for name in datasets.keys():\n",
    "    lac_coverage[name] = np.mean(all_coverage[name])\n",
    "r_value_coverage = defaultdict(list)\n",
    "for name in datasets.keys():\n",
    "    r_value_coverage[name] = np.mean(all_correct_predictions[name])\n",
    "lac_set_sizes = defaultdict(list)\n",
    "for name in datasets.keys():\n",
    "    lac_set_sizes[name] = np.mean(all_size[name])\n",
    "r_value_set_sizes = defaultdict(list)\n",
    "for name in datasets.keys():\n",
    "    r_value_set_sizes[name] = np.mean(all_sizes_rvalue[name])\n",
    "lac_using_mean_coverage = defaultdict(list)\n",
    "for name in datasets.keys():\n",
    "    lac_using_mean_coverage[name] = np.mean(all_coverage_using_mean[name])\n",
    "lac_using_mean_set_sizes = defaultdict(list)\n",
    "for name in datasets.keys():\n",
    "    lac_using_mean_set_sizes[name] = np.mean(all_size_using_mean[name])\n",
    "\n",
    "\n",
    "all_results={\n",
    "    'lac_coverage': lac_coverage,\n",
    "    'r_value_coverage': r_value_coverage,\n",
    "    'lac_set_sizes': lac_set_sizes,\n",
    "    'r_value_set_sizes': r_value_set_sizes,\n",
    "    'lac_using_mean_coverage': lac_using_mean_coverage,\n",
    "    'lac_using_mean_set_sizes': lac_using_mean_set_sizes,\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
